gateway:
  replicas: 1
  prefix: /
  loglevel: INFO
  image:
    name: geddes-registry.rcac.purdue.edu/cms/dask-gateway-server
    tag: 2023.9.0-purdue.v4
    pullPolicy: IfNotPresent
  auth:
    type: simple
    simple:
      password: null
  nodeSelector:
    cms-af-prod: "true"
  tolerations:
    - key: "hub.jupyter.org/dedicated"
      operator: "Equal"
      value: "cms-af"
      effect: "NoSchedule"
  resources:
    limits:
      cpu: 2
      memory: 4Gi
    requests:
      cpu: 2
      memory: 4Gi
  service:
    annotations:
      metallb.universe.tf/address-pool: geddes-public-pool
  env:
    - name: NAMESPACE
      value: cms
    - name: RELEASE_NAME
      value: dask-gateway-k8s-slurm
  extraConfig:
    slurmConfig: |
      import os
      RELEASE_NAME = os.environ["RELEASE_NAME"]
      NAMESPACE = os.environ["NAMESPACE"]

      c.DaskGateway.backend_class = "dask_gateway_server.backends.jobqueue.slurm.SlurmBackend"
      c.DaskGateway.address = ':8786'
      c.SlurmBackend.api_url = f'http://api-{RELEASE_NAME}.{NAMESPACE}.geddes.rcac.purdue.edu:8000/api'
      c.SlurmBackend.cluster_start_timeout = 60*5
      c.SlurmBackend.cluster_heartbeat_period = 60
      c.SlurmBackend.worker_start_timeout = 60*5
      c.ClusterConfig.idle_timeout = 3600*24

      from dask_gateway_server.options import Options, Integer, Float, Mapping, String, Select

      # ---- Pyroscope wiring (workers) ----
      # We create a preload script on the Slurm node at worker startup that
      # configures the Pyroscope Python agent and tag the profile with hostname.
      # The script path is referenced via DASK_DISTRIBUTED__WORKER__PRELOAD.
      PYROSCOPE_SERVICE = f"http://pyroscope.{NAMESPACE}.svc.cluster.local:4040"
      PRELOAD_PATH = "~/.dask-gateway/pyro_preload.py"

      def options_handler(options):
          # Base setup (keep your temp dir behavior)
          setup_cmd = "export DASK_TEMPORARY_DIRECTORY=/tmp/dask-$USER; "

          # Make sure the chosen conda env is at the front of PATH and includes headers
          options.env["PATH"] = f"{options.conda_env}/bin/:" + options.env.pop("PATH")
          options.env["CPLUS_INCLUDE_PATH"] = f"{options.conda_env}/x86_64-conda-linux-gnu/sysroot/usr/include"

          # --- Worker-specific setup for Pyroscope ---
          # 1) Ensure the preload file exists on the Slurm node.
          # 2) Ensure pyroscope-io is available in the worker env (no-op if already installed).
          worker_setup_pyroscope = f"""
          set -e
          mkdir -p $HOME/.dask-gateway
          cat > {PRELOAD_PATH} <<'PYRO'
          import os, socket
          try:
              import pyroscope
              app = os.environ.get("PYROSCOPE_APP", "dask-gateway")
              server = os.environ.get("PYROSCOPE_SERVER", "{PYROSCOPE_SERVICE}")
              pyroscope.configure(
                  application_name=f"{{app}};worker_host={{socket.gethostname()}}",
                  server_address=server,
              )
          except Exception as e:
              # Don't fail the worker if profiling can't start
              pass
          PYRO
          chmod 0644 {PRELOAD_PATH}

          if ! {options.conda_env}/bin/python -c "import pyroscope" >/dev/null 2>&1; then
              {options.conda_env}/bin/pip install --no-cache-dir --upgrade pyroscope-io >/dev/null 2>&1 || true
          fi
          """

          # Merge with your existing setup
          worker_setup = setup_cmd + worker_setup_pyroscope
          scheduler_setup = setup_cmd  # Scheduler doesn't need profiling

          # Provide env to workers: tell Dask to preload the script + point to Pyroscope
          options.env["DASK_DISTRIBUTED__WORKER__PRELOAD"] = f'["{PRELOAD_PATH}"]'
          options.env["PYROSCOPE_SERVER"] = PYROSCOPE_SERVICE
          options.env["PYROSCOPE_APP"] = "dask-gateway"

          return {
              "worker_cores": options.worker_cores,
              "worker_memory": int(options.worker_memory * 2 ** 30),
              "environment": options.env,
              "scheduler_cmd": f"{options.conda_env}/bin/dask-scheduler",
              "scheduler_partition": "hammer-nodes",
              # "scheduler_reservation": "CMSLOCAL",
              "worker_partition": "hammer-nodes",
              "worker_cmd": f"{options.conda_env}/bin/dask-worker",
              "account": "cms",
              "time": "1-00:00:00",
              "scheduler_setup": scheduler_setup,
              "worker_setup": worker_setup,
              "staging_directory": "/depot/cms/users/{username}/.dask-gateway/"
          }

      c.Backend.cluster_options = Options(
          String("conda_env", default="/depot/cms/kernels/python3", label="Conda environment"),
          Integer("worker_cores", default=1, min=1, max=16, label="Cores per worker"),
          Float("worker_memory", default=4, min=1, max=64, label="Memory per worker (GiB)"),
          # Default env you already had + Pyroscope env/preload for workers
          Mapping("env", {
              "X509_USER_PROXY": "",
              "WORKDIR": "",
              "DASK_DISTRIBUTED__WORKER__PRELOAD": f'["{PRELOAD_PATH}"]',
              "PYROSCOPE_SERVER": f"{PYROSCOPE_SERVICE}",
              "PYROSCOPE_APP": "dask-gateway",
          }, label="Environment variables"),
          handler=options_handler,
      )
traefik:
  replicas: 1
  image:
    name: geddes-registry.rcac.purdue.edu/docker-hub-cache/library/traefik
    tag: 2.10.4
    pullPolicy: IfNotPresent
  loglevel: WARN
  dashboard: false
  service:
    type: ClusterIP
    annotations:
      metallb.universe.tf/address-pool: geddes-private-pool
    ports:
      web:
        port: 80
      tcp:
        port: 8786
  nodeSelector:
    cms-af-prod: "true"
  tolerations:
    - key: "hub.jupyter.org/dedicated"
      operator: "Equal"
      value: "cms-af"
      effect: "NoSchedule"
controller:
  nodeSelector:
    cms-af-prod: "true"
  tolerations:
    - key: "hub.jupyter.org/dedicated"
      operator: "Equal"
      value: "cms-af"
      effect: "NoSchedule"
