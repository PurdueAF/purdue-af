# values-incluster.yaml
nodeName: interlink-hammer

interlink:
  # image: ghcr.io/interlink-hq/interlink/interlink:0.5.2-pre2
  image: ghcr.io/interlink-hq/interlink/interlink:latest
  enabled: true
  socket: unix:///var/run/interlink.sock
  address: http://localhost
  port: 3000
  logging:
    verboseLogging: true

plugin:
  enabled: true
  # image: "ghcr.io/interlink-hq/interlink-sidecar-slurm/interlink-sidecar-slurm:0.5.1-pre3"
  image: "geddes-registry.rcac.purdue.edu/cms/interlink-slurm-plugin:0.6.0-patch1"
  # image: ghcr.io/interlink-hq/interlink-sidecar-slurm/interlink-sidecar-slurm:0.5.2
  #socket: unix:///var/run/plugin.sock
  address: "http://localhost"
  port: 4000
  privileged: false
  extraVolumeMounts:
    - name: plugin-data
      mountPath: /depot/cms/purdue-af/interlink/
    - name: cvmfs
      mountPath: /cvmfs
  envs:
    - name: SLURMCONFIGPATH
      value: "/etc/interlink/plugin.yaml"
    - name: SHARED_FS
      values: "true"
    - name: HOME
      value: "/depot/cms/purdue-af/interlink/cache"
  config: |
    #Socket: "unix:///var/run/plugin.sock"
    ImagePrefix: "docker://"
    SidecarPort: 4000
    VerboseLogging: false
    ErrorsOnlyLogging: true
    EnableProbes: true
    # NEEDED PATH FOR GITHUB ACTIONS
    #DataRootFolder: "/home/runner/work/interLink/interLink/.interlink/"
    # on your host use something like:
    DataRootFolder: "/depot/cms/purdue-af/interlink/"
    ExportPodData: true
    SbatchPath: "/usr/bin/sbatch"
    ScancelPath: "/usr/bin/scancel"
    SqueuePath: "/usr/bin/squeue"
    CommandPrefix: ""
    ContainerRuntime: "singularity"
    SingularityPrefix: ""
    SingularityDefaultOptions:
      - "--nv"
      - "--no-eval"
    Namespace: "${namespace}"
    Tsocks: false
    TsocksPath: "$WORK/tsocks-1.8beta5+ds1/libtsocks.so"
    TsocksLoginNode: "login01"
    BashPath: /bin/bash

virtualNode:
  # image: ghcr.io/interlink-hq/interlink/virtual-kubelet-inttw:0.5.2-pre2
  image: ghcr.io/interlink-hq/interlink/virtual-kubelet-inttw:0.6.1-pre3
  resources:
    CPUs: 1000
    memGiB: 4000
    pods: 300

  disableCSR: true

  network:
    # Enable tunnel feature (creates wstunnel template ConfigMap)
    enableTunnel: true
    # Container image for wstunnel
    tunnelImage: "ghcr.io/erebe/wstunnel:latest"
    # DNS domain for ingress (e.g. each pod will have its own tunnel at:
    # "<podname>.<wildcardDNS>"
    wildcardDNS: "geddes.rcac.purdue.edu"
    # Path where wstunnel template will be mounted in VK container
    wstunnelTemplatePath: "/etc/templates/wstunnel.yaml"
    # Custom wstunnel template content (optional, uses built-in template if empty)
    # Default template is made to work with nginx ingress controller:
    #   https://github.com/interlink-hq/interlink-helm-chart/blob/main/interlink/wstunnel-template_nginx.yaml
    customTemplate: ""
    wstunnelCommand: |
      export DOWNLOAD_DIR="/depot/cms/purdue-af/interlink" && \
      curl -L https://github.com/erebe/wstunnel/releases/download/v10.4.4/wstunnel_10.4.4_linux_amd64.tar.gz -o "$DOWNLOAD_DIR/wstunnel.tar.gz" && \
      tar -xzvf "$DOWNLOAD_DIR/wstunnel.tar.gz" -C "$DOWNLOAD_DIR" && \
      chmod +x "$DOWNLOAD_DIR/wstunnel" && \
      "$DOWNLOAD_DIR/wstunnel" client --http-upgrade-path-prefix %s %s ws://%s:80 > "$DOWNLOAD_DIR/wstunnel_client.log" 2>&1 &

extraVolumes:
  - name: plugin-data
    nfs:
      server: datadepot.rcac.purdue.edu
      path: /depot/cms/purdue-af/interlink
  - name: cvmfs
    persistentVolumeClaim:
      claimName: cvmfs

podScheduling:
  nodeSelector:
    cms-af-prod: "true"
  tolerations:
    - key: "hub.jupyter.org/dedicated"
      operator: "Equal"
      value: "cms-af"
      effect: "NoSchedule"

serviceAccount:
  create: false
  name: interlink-sa
