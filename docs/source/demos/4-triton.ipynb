{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c823e-6fe1-4fe8-adc5-0d51890db1e7",
   "metadata": {},
   "source": [
    "# Demo 4: Accelerating ML inference using Triton inference servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56e44d-019a-4583-9ba8-691686cd851c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this demo we show how analysis workflows can be accelerated by outsourcing the ML inference to Triton servers with GPUs.\n",
    "\n",
    "## 1. Loading events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7568c-e128-49e8-99f5-d50973e657b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from submodule.event_selection import load_events\n",
    "from submodule.dnn_model import NeuralNet\n",
    "\n",
    "sources = [\"data\", \"ttbar\", \"dy\"]\n",
    "server = \"file:/depot/cms/purdue-af/demos/\"\n",
    "model_dir = \"/depot/cms/purdue-af/demos/\"\n",
    "dfs = {}\n",
    "\n",
    "features = ['mu1_pt', 'mu1_eta', 'mu2_pt', 'mu2_eta', 'dimuon_mass', 'met']\n",
    "\n",
    "# load datasets for inference\n",
    "for src in sources:\n",
    "    dfs[src] = load_events(f\"{server}/{src}.root\")[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d0873-9644-47cd-aa4e-76bf619f8dfe",
   "metadata": {},
   "source": [
    "## 2. Outsourcing ML inference to remote GPUs via Triton servers\n",
    "Machine learning inference is known to run much faster on GPUs as compared to CPUs. However, computing clusters are usually limited in number of GPUs, therefore it is not possible to ensure full access to GPUs for all users at all times.\n",
    "\n",
    "An approach allowing to use the power of GPUs to accelerate inference without blocking the GPU nodes is to use dedicated inference servers which are always connected to GPUs.\n",
    "\n",
    "In order to be able to evaluate a model via a Triton server, the model has to be saved in a special way: [see example how to do that in PyTorch](https://medium.com/@furcifer/deploying-triton-inference-server-in-5-minutes-67aa09a84ca6).\n",
    "\n",
    "The saved models must be put into a repository visible to the Triton server(s). At the moment, we host the repository at Purdue shared project storage (Depot): `/depot/cms/purdue-af/triton/models/`. In the future, a repository with write access for non-Purdue users will be set up as well.\n",
    "\n",
    "At the moment, we provide several Triton servers corresponding to different GPUs / GPU partitions. To select a particular server, simply uncomment the corresponding address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5c7cb-636f-4b96-a833-2b84ed7f1c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_address = 'triton-10gb.cms.geddes.rcac.purdue.edu:8001'\n",
    "# Triton load balancer running at the partition of A100 GPU with 10gb RAM\n",
    "\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "print(f\"Connecting to Triton inference sever at {triton_address}\")\n",
    "\n",
    "keepalive_options = grpcclient.KeepAliveOptions(\n",
    "    keepalive_time_ms=2**31 - 1,\n",
    "    keepalive_timeout_ms=20000,\n",
    "    keepalive_permit_without_calls=False,\n",
    "    http2_max_pings_without_data=2\n",
    ")\n",
    "\n",
    "# Create Triton client\n",
    "try:\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url=triton_address,\n",
    "        verbose=False,\n",
    "        keepalive_options=keepalive_options\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Channel creation failed: \" + str(e))\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327803d1-3aac-47fa-b273-8245b2df80b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_triton(inp):\n",
    "    label= inp[0]\n",
    "    df = inp[1]\n",
    "    \n",
    "    # Inputs and outputs should be compatible with model metadata\n",
    "    # stored in /depot/cms/purdue-af/triton/models/test-model/config.pbtxt\n",
    "    inputs = [grpcclient.InferInput('INPUT__0', df.shape, \"FP64\")]\n",
    "    outputs = [grpcclient.InferRequestedOutput('OUTPUT__0')]\n",
    "    \n",
    "    # Load input data\n",
    "    inputs[0].set_data_from_numpy(df.values)\n",
    "    \n",
    "    # Run inference on Triton server.\n",
    "    # Models are stored in /depot/cms/purdue-af/triton/models/\n",
    "    results = triton_client.infer(\n",
    "        model_name=\"test-model\",\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        headers={'test': '1'},\n",
    "    )\n",
    "    scores = results.as_numpy('OUTPUT__0').flatten()\n",
    "    \n",
    "    # Save DNN outputs to a file\n",
    "    save_path = f\"/depot/cms/users/dkondra/dnn_outputs_triton/{label}.npy\"\n",
    "    np.save(save_path, scores, allow_pickle=True)\n",
    "    print(label, scores)\n",
    "\n",
    "\n",
    "print(\"\\nDatasets:\", list(dfs.keys()))\n",
    "results = []\n",
    "for label, df in dfs.items():\n",
    "    inference_triton([label, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10a999-1f08-4928-8a7a-99d8091af1b4",
   "metadata": {},
   "source": [
    "## Plotting DNN outputs\n",
    "Run this cell after either Dask parallelization example or after Triton example to plot the DNN outputs (note that the models are different in these examples, so the outputs will not look the same). The models are generic and not meant to provide any physics meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f337c-d48c-438f-b78b-a3837c3d5d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "dnn_outputs = {}\n",
    "\n",
    "for src in sources:\n",
    "    load_path = f\"/depot/cms/users/dkondra/dnn_outputs_triton/{src}.npy\"\n",
    "    dnn_outputs[src] = np.load(load_path)\n",
    "\n",
    "plt.hist(dnn_outputs[\"dy\"], bins, alpha=0.3, label='dy', density=True)\n",
    "plt.hist(dnn_outputs[\"ttbar\"], bins, alpha=0.3, label='ttbar', density=True)\n",
    "plt.hist(dnn_outputs[\"data\"], bins, alpha=0.3, label='data', density=True)\n",
    "plt.xlabel('DNN Score')\n",
    "plt.ylabel('Events')\n",
    "leg = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d01064-e16d-4317-91e9-96a42fa4a956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel [ML]",
   "language": "python",
   "name": "python3-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
