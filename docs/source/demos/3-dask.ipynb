{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c823e-6fe1-4fe8-adc5-0d51890db1e7",
   "metadata": {},
   "source": [
    "# Demo 3: Scaling up Python workflows using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4faad-bd6b-4f28-af55-585d2f2c4fa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this demo we show how analysis workflows can be scaled to multiple CPUs by parallelizing the processing using `dask.distributed` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3401b2e-f237-45b6-8369-c5c639b56d1b",
   "metadata": {},
   "source": [
    "## 1. Creating a Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524713c-b629-40d3-bb3f-bb5a13bc5724",
   "metadata": {},
   "source": [
    "A **Dask cluster** consists of a **scheduler** that can spawn multiple **workers**, to which the workload can be distributed.\n",
    "\n",
    "### 1.1 Types of Dask clusters\n",
    "The types of clusters available in the Analysis Facility are:\n",
    "- `LocalCluster` - uses local CPUs (inside the user's AF session).\n",
    "- `PurdueSLURMCluster` - custom SLURM cluster that distributes the workload to SLURM batch system on Purdue CMS (Hammer) cluster.\n",
    "\n",
    "We recommend to use the `SLURM` cluster, as it comes with useful features:\n",
    "- You can specify a kernel that all workers will use\n",
    "- SLURM cluster has access to much more CPUs and GPUs, compared to LocalCluster\n",
    "\n",
    "### 1.2 Ensuring consistent environment between analysis workflow and Dask workers\n",
    "It is crucial that the Dask workers have access to the software packages used in the analysis. Due to the way Dask operates, this includes not only the packages used in the part of the code executed by workers, but *all* packages used in the workflow.\n",
    "\n",
    "This includes the following:\n",
    "- **Python packages**: Dask cluster should be launched with the same kernel (Conda environment) as the analysis workflow (Jupyter notebook or Python script). The kernel (Conda environment) should be visible to the workers, the best way to ensure that is to store it in **Depot**.\n",
    "- **Local imports** (from other Python files in your analysis framework.): this can be done by *explicitly uploading your code to the workers* (see Section 1.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da5e16-1e55-4afd-9e6e-73c6105c19e8",
   "metadata": {},
   "source": [
    "### 1.3 Starting a Dask cluster\n",
    "- Open Dask extension (click on red logo in the left sidebar)\n",
    "- Click on [+ NEW] button\n",
    "- In the dialogue window, select `SLURM cluster` and `Python3 kernel [ML]`\n",
    "- Select desired number of workers\n",
    "- Once the cluster is created, copy its IP address and paste below to connect a Dask client to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d611bed-1de1-4d4b-8776-e074b580b031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.5.15.122:8786\") # paste correct IP address here\n",
    "# print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4cce7-33d4-4a84-a1ef-06fbb306e5ae",
   "metadata": {},
   "source": [
    "### 1.4 Upload your code to workers\n",
    "The code below shows how to upload the local code to workers. This is needed to ensure that the workers understand imports like `from submodule.event_selection import load_events`.\n",
    "\n",
    "**Run the following cell, then restart this notebook, reconnect the Dask client to the SLURM cluster, and proceed with other cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65657f3-5358-4ca3-ab23-0281b4362eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distributed.diagnostics.plugin import UploadDirectory\n",
    "client.register_worker_plugin(UploadDirectory(\"./\", restart=True, update_path=True))\n",
    "client.register_worker_plugin(UploadDirectory(\"submodule\", restart=True, update_path=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cbba0-9480-4d6f-8c5b-b6cb67f605d1",
   "metadata": {},
   "source": [
    "## 2. Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca4780-24dc-4657-9cf2-b646efe978b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before running this section:\n",
    "- Create a Dask SLURM cluster in the extension\n",
    "- Create a client as shown in Section 1.3\n",
    "- Upload code to workers as shown in Section 1.4\n",
    "- Restart the kernel\n",
    "- Run Section 1.3 again to reconnect the client to the cluster\n",
    "\n",
    "Here we load the pre-selected NanoAOD events using `uproot` (see code in `submodule/event_selection.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7568c-e128-49e8-99f5-d50973e657b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from submodule.event_selection import load_events\n",
    "from submodule.dnn_model import NeuralNet\n",
    "\n",
    "sources = [\"data\", \"ttbar\", \"dy\"]\n",
    "server = \"file:/depot/cms/purdue-af/demos/\"\n",
    "model_dir = \"/depot/cms/purdue-af/demos/\"\n",
    "dfs = {}\n",
    "\n",
    "features = ['mu1_pt', 'mu1_eta', 'mu2_pt', 'mu2_eta', 'dimuon_mass', 'met']\n",
    "\n",
    "# load datasets for inference\n",
    "for src in sources:\n",
    "    dfs[src] = load_events(f\"{server}/{src}.root\")[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29e3fe-048d-4f8a-91ae-698bbb903d36",
   "metadata": {},
   "source": [
    "## 3. Example parallelization\n",
    "In order to test the distributed processing setup, we run a simple DNN inference for three small datasets in parallel. Here DNN inference is just an example of a processing code that can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c9940-6894-44ba-a4b3-387cbc1458ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if there are any local GPUs available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Will use GPU for inference.\")\n",
    "else:\n",
    "    print(\"Will use CPUs for inference.\")\n",
    "\n",
    "\n",
    "# The main processing function that will be executed in parallel on multiple datasets\n",
    "def inference(inp):\n",
    "    label = inp[0]\n",
    "    df = inp[1]\n",
    "    model_path=model_dir+\"/model.ckpt\"\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = NeuralNet(6, [16, 8], 1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    df = torch.from_numpy(df.values).to(device).float()\n",
    "    scores = model(df) \n",
    "    scores = scores.cpu().detach().numpy().ravel()\n",
    "    \n",
    "    # Save DNN outputs to a file\n",
    "    save_path = f\"/depot/cms/users/dkondra/dnn_outputs/{label}.npy\"\n",
    "    np.save(save_path, scores, allow_pickle=True)\n",
    "    return label, scores\n",
    "\n",
    "\n",
    "print(\"\\nDatasets:\", list(dfs.keys()))\n",
    "# Distribute the datasets to workers\n",
    "scattered_data = client.scatter(list(dfs.items()))\n",
    "\n",
    "# Process the datasets in parallel and return the results\n",
    "futures = client.map(inference, scattered_data)\n",
    "results = client.gather(futures)\n",
    "\n",
    "for res in results:\n",
    "    print(res[0], res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10a999-1f08-4928-8a7a-99d8091af1b4",
   "metadata": {},
   "source": [
    "## 4. Plotting outputs\n",
    "Run this cell after either Dask parallelization example or after Triton example to plot the DNN outputs (note that the models are different in these examples, so the outputs will not look the same). The models are generic and not meant to provide any physics meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f337c-d48c-438f-b78b-a3837c3d5d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "dnn_outputs = {}\n",
    "\n",
    "for src in sources:\n",
    "    load_path = f\"/depot/cms/users/dkondra/dnn_outputs/{src}.npy\"\n",
    "    dnn_outputs[src] = np.load(load_path)\n",
    "\n",
    "plt.hist(dnn_outputs[\"dy\"], bins, alpha=0.3, label='dy', density=True)\n",
    "plt.hist(dnn_outputs[\"ttbar\"], bins, alpha=0.3, label='ttbar', density=True)\n",
    "plt.hist(dnn_outputs[\"data\"], bins, alpha=0.3, label='data', density=True)\n",
    "plt.xlabel('DNN Score')\n",
    "plt.ylabel('Events')\n",
    "leg = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872a801-521a-41c6-816f-1525fe47f187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel [ML]",
   "language": "python",
   "name": "python3-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
